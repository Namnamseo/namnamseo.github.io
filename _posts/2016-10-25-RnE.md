---
layout: post
title: 161025 일지
category: [2016 R&E]
---



*16. 10. 25.*

# 오전

## Test of single-layer network without pooling in between

### ReLU

[prototxt](http://codepad.org/VMDhJlun).

#### SGD 1e-03 (fixed)

이미지를 넣어 테스트.

```
Layer data_scaled: shape (1, 1, 105, 105)
[[[[ 0.84765625  0.84765625  0.81640625 ...,  0.87890625  0.86328125
     0.88671875]
   [ 0.84765625  0.84765625  0.81640625 ...,  0.87890625  0.86328125
     0.88671875]
   [ 0.87109375  0.87109375  0.83984375 ...,  0.86328125  0.8515625
     0.86328125]
   ...,
   [ 0.83984375  0.83984375  0.79296875 ...,  0.88671875  0.890625
     0.8828125 ]
   [ 0.84375     0.84375     0.8203125  ...,  0.85546875  0.859375
     0.86328125]
   [ 0.84375     0.84375     0.8359375  ...,  0.84375     0.86328125
     0.87109375]]]]

Layer conv1: shape (1, 64, 48, 48)
[[[[ -2.16158374e+03  -2.15226831e+03  -2.15911011e+03 ...,
     -2.17850806e+03  -2.20590894e+03  -2.22350806e+03]
   [ -2.18993457e+03  -2.18599243e+03  -2.18573657e+03 ...,
     -2.17065796e+03  -2.19286450e+03  -2.20655322e+03]
   [ -2.20037671e+03  -2.19805957e+03  -2.19645044e+03 ...,
     -2.16636108e+03  -2.19009912e+03  -2.20385474e+03]
   ...,
   [ -2.10513867e+03  -2.11640771e+03  -2.12364551e+03 ...,
     -2.28382373e+03  -2.27643359e+03  -2.26202344e+03]
   [ -2.11971167e+03  -2.12899561e+03  -2.13474512e+03 ...,
     -2.23196411e+03  -2.23256860e+03  -2.23545361e+03]
   [ -2.14211792e+03  -2.14644922e+03  -2.14709424e+03 ...,
     -2.18839209e+03  -2.19695776e+03  -2.21110815e+03]], ...]]

Layer act1: shape (1, 64, 48, 48)
[[[[ 0.  0.  0. ...,  0.  0.  0.]
   [ 0.  0.  0. ...,  0.  0.  0.]
   [ 0.  0.  0. ...,  0.  0.  0.]
   ...,
   [ 0.  0.  0. ...,  0.  0.  0.]
   [ 0.  0.  0. ...,  0.  0.  0.]
   [ 0.  0.  0. ...,  0.  0.  0.]], ...]]

Layer deconv4: shape (1, 1, 105, 105)
[[[[ -3.05649906e+05  -5.59184188e+05  -6.25507875e+05 ...,
     -7.61531000e+05  -6.56312312e+05  -3.77599688e+05]
   [ -1.72505075e+06  -9.87120845e+09  -1.35423680e+07 ...,
     -6.13116273e+10  -1.08354028e+10  -3.07640750e+10]
   [ -6.26828000e+05  -1.14822688e+06  -1.27904825e+06 ...,
     -1.52680762e+06  -1.33048512e+06  -7.51739375e+05]
   ...,
   [ -5.08847031e+05  -1.14493800e+06  -1.07546625e+06 ...,
     -1.40005812e+06  -1.40384962e+06  -7.05425750e+05]
   [ -5.40688920e+07  -9.34311219e+09  -1.09659536e+08 ...,
     -1.07366344e+08  -1.03746253e+10  -5.28277000e+07]
   [ -2.46210812e+05  -5.65744312e+05  -5.24787062e+05 ...,
     -6.54697438e+05  -6.88095812e+05  -3.29530406e+05]]]]

Layer act4: shape (1, 1, 105, 105)
[[[[ 0.  0.  0. ...,  0.  0.  0.]
   [ 0.  0.  0. ...,  0.  0.  0.]
   [ 0.  0.  0. ...,  0.  0.  0.]
   ...,
   [ 0.  0.  0. ...,  0.  0.  0.]
   [ 0.  0.  0. ...,  0.  0.  0.]
   [ 0.  0.  0. ...,  0.  0.  0.]]]]

Layer flat_out: shape (1, 11025)
[[ 0.  0.  0. ...,  0.  0.  0.]]

Layer loss_euclid: shape ()
3080.00683594
```

conv1의 값이 모두 음수이고, 이것이 act1의 ReLU에 의해 0이 되고, deconv에서는 때문에 bias만 남는데 그 bias들도 모두 음수이고, 결과적으로 act4의 값 또한 0이 되므로, 모든 값이 0이 되고, gradient가 0이므로 학습이 되지 않는 것으로 보인다.

#### SGD 1e-04 (fixed)

```
Layer conv1: shape (1, 64, 48, 48)
[[[[  610.67767334   608.13586426   610.13690186 ...,   615.97332764
      623.74523926   628.68359375]
   [  618.77728271   617.62335205   617.58886719 ...,   613.57092285
      620.02813721   623.84667969]
   [  621.81555176   621.152771     620.60693359 ...,   612.1227417
      618.9576416    622.96411133]
   ...,
   [  594.76867676   598.04016113   600.1619873  ...,   645.54266357
      643.45410156   639.43981934]
   [  599.10321045   601.79101562   603.3503418  ...,   630.87286377
      631.1449585    631.93365479]
   [  605.38665771   606.61633301   606.83740234 ...,   618.50469971
      621.19024658   625.17718506]]

  [[ -945.48223877  -941.75317383  -944.85687256 ...,  -953.61895752
     -965.63153076  -973.08300781]
   [ -958.04199219  -956.41741943  -956.28210449 ...,  -949.8838501
     -959.79144287  -965.70507812]
   [ -962.64654541  -961.73120117  -960.93939209 ...,  -947.85028076
     -958.35778809  -964.41760254]
   ...,
   [ -920.92034912  -926.08831787  -929.37805176 ...,  -998.68243408
     -995.62353516  -989.36846924]
   [ -927.54882812  -931.67431641  -934.14593506 ...,  -976.19140625
     -976.74664307  -978.10015869]
   [ -937.22314453  -939.04571533  -939.37438965 ...,  -957.32244873
     -961.60247803  -967.81512451]]

  [[-2392.08911133 -2382.921875   -2391.04907227 ..., -2413.78393555
    -2443.97314453 -2462.68237305]
   [-2424.18969727 -2420.20214844 -2419.75927734 ..., -2404.2565918
    -2429.28320312 -2444.11157227]
   [-2435.92895508 -2433.73242188 -2431.60302734 ..., -2398.79101562
    -2425.51293945 -2440.85327148]
   ..., 
   [-2330.07275391 -2343.46191406 -2351.90771484 ..., -2527.14257812
    -2519.54418945 -2503.50439453]
   [-2347.08984375 -2357.70776367 -2363.93798828 ..., -2470.38818359
    -2472.10766602 -2475.25341797]
   [-2371.7565918  -2376.51904297 -2377.19042969 ..., -2422.81982422
    -2433.88891602 -2449.48876953]]

  [[    7.52396011     7.52261019     7.5396204  ...,     7.68146944
        7.72782087     7.74057531]
   [    7.60796165     7.64076233     7.66247177 ...,     7.6427331
        7.68027735     7.70111418]
   [    7.67072439     7.6305337      7.68667173 ...,     7.61217499
        7.67792225     7.68881416]
   ..., 
   [    7.40242767     7.42484331     7.45579815 ...,     7.79314232
        7.92093945     7.82670021]
   [    7.3419838      7.43909693     7.47848511 ...,     7.75689316
        7.76663113     7.78232622]
   [    7.46863985     7.495121       7.50546408 ...,     7.6584816
        7.65920544     7.77282763]]]]

...

Layer deconv4: shape (1, 1, 105, 105)
[[[[ -8.50186218e+02  -1.06065475e+02  -1.91414111e+03 ...,
     -9.93990723e+02  -2.17835236e+02  -2.80042511e+02]
   [ -4.18859283e+02  -2.24875734e+05  -8.36879272e+02 ...,
     -6.82701477e+02  -2.27828312e+05  -3.46402771e+02]
   [ -1.89376245e+03  -3.28325653e+02  -4.64084229e+03 ...,
     -2.75124805e+03  -4.23788788e+02  -9.31595276e+02]
   ..., 
   [ -3.03544141e+03  -5.76754639e+02  -6.40490332e+03 ...,
     -4.50203564e+03  -5.91576843e+02  -1.56014136e+03]
   [ -9.43599487e+02  -2.26149641e+05  -1.30355896e+03 ...,
     -5.82862000e+02  -2.29843172e+05  -3.26015411e+02]
   [ -1.17291443e+03  -2.68605408e+02  -2.44744067e+03 ...,
     -1.44160352e+03  -4.51753357e+02  -3.97661682e+02]]]]

Layer act4: shape (1, 1, 105, 105)
[[[[ 0.  0.  0. ...,  0.  0.  0.]
   [ 0.  0.  0. ...,  0.  0.  0.]
   [ 0.  0.  0. ...,  0.  0.  0.]
   ..., 
   [ 0.  0.  0. ...,  0.  0.  0.]
   [ 0.  0.  0. ...,  0.  0.  0.]
   [ 0.  0.  0. ...,  0.  0.  0.]]]]

Layer flat_out: shape (1, 11025)
[[ 0.  0.  0. ...,  0.  0.  0.]]

Layer loss_euclid: shape ()
3080.00683594
```

역시 값들이 0이 되는 모습을 볼 수 있었다.

### TanH

#### SGD 1e-03

```
Layer conv1: shape (1, 64, 48, 48)
[[[[  34.67094421   34.44452286   34.74466705 ...,   36.30485916
      36.70415878   36.98782349]
   [  35.53877258   35.31368256   35.44117737 ...,   36.12796021
      36.51347733   36.73820877]
   [  35.8000145    35.68549728   35.72744751 ...,   35.66474915
      36.10268402   36.30751038]
   ...,
   [  33.80752182   34.02541351   34.25660706 ...,   39.5677948
      38.80934906   37.95577621]
   [  34.38694382   34.70754242   34.88232803 ...,   38.02596283
      37.51864624   36.98058319]
   [  35.35146713   35.5990448    35.50596619 ...,   36.41238022
      36.40263748   36.40491486]]

  [[ 193.82241821  192.89440918  193.42570496 ...,  194.95739746
     197.46098328  199.13017273]
   [ 196.22799683  195.86112976  195.87141418 ...,  194.2878418
     196.31430054  197.54849243]
   [ 197.22042847  196.92976379  196.80140686 ...,  193.98074341
     196.07312012  197.28955078]
  [[  25.45172691   25.34275055   25.47091866 ...,   25.59456062
      25.94911766   26.14885712]
   [  25.7776947    25.75918388   25.72390938 ...,   25.56400681
      25.79692841   25.97075653]
   [  25.8792057    25.87667465   25.8409977  ...,   25.51127434
      25.8813343    25.98613358]
   ...,
   [  24.80511284   24.86919022   24.98794556 ...,   26.73645782
      26.63446426   26.54986954]
   [  24.89356804   25.01849174   25.0764389  ...,   26.26563072
      26.28157043   26.25727463]
   [  25.1886673    25.22442818   25.26716614 ...,   25.82378197
      25.86419487   25.99577522]]]]


Layer act1: shape (1, 64, 48, 48)
[[[[ 1.  1.  1. ...,  1.  1.  1.]
   [ 1.  1.  1. ...,  1.  1.  1.]
   [ 1.  1.  1. ...,  1.  1.  1.]
   ...,
   [ 1.  1.  1. ...,  1.  1.  1.]
   [ 1.  1.  1. ...,  1.  1.  1.]
   [ 1.  1.  1. ...,  1.  1.  1.]]

  [[ 1.  1.  1. ...,  1.  1.  1.]
   [ 1.  1.  1. ...,  1.  1.  1.]
   [ 1.  1.  1. ...,  1.  1.  1.]
   ...,
   [ 1.  1.  1. ...,  1.  1.  1.]
   [ 1.  1.  1. ...,  1.  1.  1.]
   [ 1.  1.  1. ...,  1.  1.  1.]]

  [[ 1.  1.  1. ...,  1.  1.  1.]
   [ 1.  1.  1. ...,  1.  1.  1.]
   [ 1.  1.  1. ...,  1.  1.  1.]
   ...,
   [ 1.  1.  1. ...,  1.  1.  1.]
   [ 1.  1.  1. ...,  1.  1.  1.]
   [ 1.  1.  1. ...,  1.  1.  1.]]

  ...,
  [[-1. -1. -1. ..., -1. -1. -1.]
   [-1. -1. -1. ..., -1. -1. -1.]
   [-1. -1. -1. ..., -1. -1. -1.]
   ...,
   [-1. -1. -1. ..., -1. -1. -1.]
   [-1. -1. -1. ..., -1. -1. -1.]
   [-1. -1. -1. ..., -1. -1. -1.]]

  [[ 1.  1.  1. ...,  1.  1.  1.]
   [ 1.  1.  1. ...,  1.  1.  1.]
   [ 1.  1.  1. ...,  1.  1.  1.]
   ...,
   [ 1.  1.  1. ...,  1.  1.  1.]
   [ 1.  1.  1. ...,  1.  1.  1.]
   [ 1.  1.  1. ...,  1.  1.  1.]]

  [[ 1.  1.  1. ...,  1.  1.  1.]
   [ 1.  1.  1. ...,  1.  1.  1.]
   [ 1.  1.  1. ...,  1.  1.  1.]
   ...,
   [ 1.  1.  1. ...,  1.  1.  1.]
   [ 1.  1.  1. ...,  1.  1.  1.]
   [ 1.  1.  1. ...,  1.  1.  1.]]]]

Layer deconv4: shape (1, 1, 105, 105)
[[[[  60.79759598   23.31859589   96.3888092  ...,   48.92546082
      23.028368     34.72576141]
   [  36.66046906   25.90437508   62.34167099 ...,   58.61101913
      12.20089912   34.73313904]
   [ 107.80865479   34.97294617  175.26031494 ...,   78.76759338
      34.8670311    54.17470932]
   ...,
   [ 111.55664062   40.3970871   184.08067322 ...,  100.72982788
      39.58291626   66.29911804]
   [  25.93530273   36.4466095    42.83680725 ...,   39.73918152
      23.48257446   25.47775078]
   [  60.52057648   25.46233368   95.79161072 ...,   56.44615555
      25.85441017   38.7723465 ]]]]

Layer act4: shape (1, 1, 105, 105)
[[[[ 1.  1.  1. ...,  1.  1.  1.]
   [ 1.  1.  1. ...,  1.  1.  1.]
   [ 1.  1.  1. ...,  1.  1.  1.]
   ...,
   [ 1.  1.  1. ...,  1.  1.  1.]
   [ 1.  1.  1. ...,  1.  1.  1.]
   [ 1.  1.  1. ...,  1.  1.  1.]]]]

Layer flat_out: shape (1, 11025)
[[ 1.  1.  1. ...,  1.  1.  1.]]

Layer loss_euclid: shape ()
1203.58154297
```

activation function인 TanH에 들어가는 x값의 절댓값이 너무 커서, 모든 activation 값이 1 혹은 -1로 나온다.

### Leaky ReLU

#### SGD, 1e-06

수렴은 굉장히 잘 하고, 예시도 잘 나온다.

in

![img](https://namnamseo.github.io/img/2016-10-25-RnE/in.png)

out

![img](https://namnamseo.github.io/img/2016-10-25-RnE/out.png)

하지만 filter는 제대로 학습되지 않은 것 같았다.

conv1

![img](https://namnamseo.github.io/img/2016-10-25-RnE/conv1.png)

deconv4

![img](https://namnamseo.github.io/img/2016-10-25-RnE/deconv4.png)

또한 learning rate가 `1e-5` 이상이면 NaN이 발생한다. 이건 왜일까...

# 오후

## 할 일

- confusion matrix
- clustering

으